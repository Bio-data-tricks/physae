{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysAE data generation walkthrough\n",
    "\n",
    "This notebook documents a compact scenario for building synthetic spectra with `SpectraDataset` while keeping track of the clean and noisy normalisation steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "\n",
    "This workflow covers:\n",
    "- Loading the baseline data configuration and narrowing the training domain.\n",
    "- Cloning and trimming validation ranges so they remain strict subsets of the training space.\n",
    "- Registering the normalisation parameters used by `SpectraDataset`.\n",
    "- Instantiating train and validation datasets with consistent noise settings.\n",
    "- Inspecting individual samples with denormalised parameters.\n",
    "- Visualising the clean and noisy spectra before and after normalisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Dict, Iterable, Mapping, Tuple\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError as exc:\n",
    "    raise RuntimeError(\"PyTorch is required to run this notebook end-to-end.\") from exc\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as exc:\n",
    "    raise RuntimeError(\"Matplotlib is required for the visualisations.\") from exc\n",
    "\n",
    "from physae.config_loader import load_data_config\n",
    "from physae import config as physae_config\n",
    "from physae.config import PARAMS, assert_subset\n",
    "from physae.dataset import SpectraDataset\n",
    "from physae.normalization import unnorm_param_tensor\n",
    "from physae.physics import batch_physics_forward_multimol_vgrid, parse_csv_transitions\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "def to_interval_dict(mapping: Mapping[str, Iterable[float]]) -> Dict[str, Tuple[float, float]]:\n",
    "    return {name: (float(bounds[0]), float(bounds[1])) for name, bounds in mapping.items()}\n",
    "\n",
    "def display_ranges(title: str, ranges: Mapping[str, Tuple[float, float]]) -> None:\n",
    "    print(title)\n",
    "    for name, (lo, hi) in ranges.items():\n",
    "        print(f\"  {name:>9}: [{lo:.6g}, {hi:.6g}]\")\n",
    "\n",
    "def normalise_noise_cfg(cfg: Mapping[str, object]) -> Dict[str, object]:\n",
    "    result: Dict[str, object] = {}\n",
    "    for key, value in cfg.items():\n",
    "        if isinstance(value, list):\n",
    "            result[key] = tuple(float(v) for v in value)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    return result\n",
    "\n",
    "def denorm_params(params_tensor: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "    values: Dict[str, torch.Tensor] = {}\n",
    "    for idx, name in enumerate(PARAMS):\n",
    "        values[name] = unnorm_param_tensor(name, params_tensor[..., idx])\n",
    "    return values\n",
    "\n",
    "def regenerate_clean_spectrum(\n",
    "    params_phys: Mapping[str, float],\n",
    "    *,\n",
    "    num_points: int,\n",
    "    poly_freq_CH4,\n",
    "    transitions_dict,\n",
    ") -> torch.Tensor:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.float32\n",
    "    sig0 = torch.tensor([params_phys['sig0']], dtype=dtype, device=device)\n",
    "    dsig = torch.tensor([params_phys['dsig']], dtype=dtype, device=device)\n",
    "    mf_CH4 = torch.tensor([params_phys['mf_CH4']], dtype=dtype, device=device)\n",
    "    baseline_coeffs = torch.tensor([[\n",
    "        params_phys['baseline0'],\n",
    "        params_phys['baseline1'],\n",
    "        params_phys['baseline2'],\n",
    "    ]], dtype=dtype, device=device)\n",
    "    pressure = torch.tensor([params_phys['P']], dtype=dtype, device=device)\n",
    "    temperature = torch.tensor([params_phys['T']], dtype=dtype, device=device)\n",
    "    v_grid_idx = torch.arange(num_points, dtype=dtype, device=device)\n",
    "    spectra_clean, _ = batch_physics_forward_multimol_vgrid(\n",
    "        sig0,\n",
    "        dsig,\n",
    "        poly_freq_CH4,\n",
    "        v_grid_idx,\n",
    "        baseline_coeffs,\n",
    "        transitions_dict,\n",
    "        pressure,\n",
    "        temperature,\n",
    "        {'CH4': mf_CH4},\n",
    "        device=device,\n",
    "    )\n",
    "    return spectra_clean[0].detach().cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the default data configuration\n",
    "\n",
    "We start from the packaged `default` scenario and create a copy that we can modify without mutating the YAML-backed dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cfg = load_data_config(name='default')\n",
    "custom_cfg = deepcopy(base_cfg)\n",
    "\n",
    "base_train = to_interval_dict(custom_cfg['train_ranges_base'])\n",
    "base_val = to_interval_dict(custom_cfg['val_ranges'])\n",
    "display_ranges('Baseline train ranges', base_train)\n",
    "display_ranges('Baseline validation ranges', base_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a narrower training domain\n",
    "\n",
    "The experiment below focuses on a tighter region of the parameter space. We override the training ranges accordingly while keeping the structure consistent with the YAML configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrow_train_ranges = {\n",
    "    'sig0': (3085.435, 3085.447),\n",
    "    'dsig': (0.0015235, 0.0015335),\n",
    "    'mf_CH4': (5.0e-06, 1.4e-05),\n",
    "    'baseline0': (0.995, 1.005),\n",
    "    'baseline1': (-0.00038, -0.00031),\n",
    "    'baseline2': (-3.8e-08, -3.2e-08),\n",
    "    'P': (450.0, 550.0),\n",
    "    'T': (305.0, 309.0),\n",
    "}\n",
    "custom_cfg['train_ranges'] = {name: [float(lo), float(hi)] for name, (lo, hi) in narrow_train_ranges.items()}\n",
    "display_ranges('Custom train ranges', narrow_train_ranges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clamp validation ranges inside the new training domain\n",
    "\n",
    "After shrinking the training region we clone the validation ranges and clamp every interval (including `mf_CH4`) so that it remains a strict subset of the training space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_val = deepcopy(custom_cfg.get('val_ranges', {}))\n",
    "adjusted_val = {}\n",
    "for name, train_interval in custom_cfg['train_ranges'].items():\n",
    "    train_min, train_max = map(float, train_interval)\n",
    "    val_min, val_max = map(float, original_val.get(name, train_interval))\n",
    "    adj_min = max(val_min, train_min)\n",
    "    adj_max = min(val_max, train_max)\n",
    "    if adj_min > adj_max:\n",
    "        centre = 0.5 * (train_min + train_max)\n",
    "        adj_min = adj_max = centre\n",
    "    adjusted_val[name] = [adj_min, adj_max]\n",
    "custom_cfg['val_ranges'] = adjusted_val\n",
    "val_ranges = to_interval_dict(custom_cfg['val_ranges'])\n",
    "assert_subset(val_ranges, narrow_train_ranges, 'validation', 'train')\n",
    "display_ranges('Adjusted validation ranges', val_ranges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Register normalisation parameters\n",
    "\n",
    "`SpectraDataset` pulls its parameter normalisation bounds from `physae.config.NORM_PARAMS`. We therefore register the updated training ranges before instantiating any dataset objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physae_config.set_norm_params({name: (float(lo), float(hi)) for name, (lo, hi) in narrow_train_ranges.items()})\n",
    "physae_config.get_norm_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate train and validation datasets\n",
    "\n",
    "With the ranges and normalisation in place we build both datasets. The validation split reuses the same sampling logic but freezes noise for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(int(custom_cfg.get('seed', 42)))\n",
    "\n",
    "poly_freq_CH4 = [-2.3614803e-07, 1.2103413e-10, -3.1617856e-14]\n",
    "transitions_ch4_str = \"\"\"\n",
    "6;1;3085.861015;1.013E-19;0.06;0.078;219.9411;0.73;-0.00712;0.0;0.0221;0.96;0.584;1.12\n",
    "6;1;3085.832038;1.693E-19;0.0597;0.078;219.9451;0.73;-0.00712;0.0;0.0222;0.91;0.173;1.11\n",
    "6;1;3085.893769;1.011E-19;0.0602;0.078;219.9366;0.73;-0.00711;0.0;0.0184;1.14;-0.516;1.37\n",
    "6;1;3086.030985;1.659E-19;0.0595;0.078;219.9197;0.73;-0.00711;0.0;0.0193;1.17;-0.204;0.97\n",
    "6;1;3086.071879;1.000E-19;0.0585;0.078;219.9149;0.73;-0.00703;0.0;0.0232;1.09;-0.0689;0.82\n",
    "6;1;3086.085994;6.671E-20;0.055;0.078;219.9133;0.70;-0.00610;0.0;0.0300;0.54;0.00;0.0\n",
    "\"\"\"\n",
    "transitions_dict = {'CH4': parse_csv_transitions(transitions_ch4_str)}\n",
    "\n",
    "n_points = int(custom_cfg.get('n_points', 800))\n",
    "n_train = int(custom_cfg.get('n_train', 50000))\n",
    "n_val = int(custom_cfg.get('n_val', 5000))\n",
    "train_ranges = narrow_train_ranges\n",
    "val_ranges = to_interval_dict(custom_cfg['val_ranges'])\n",
    "noise_train = normalise_noise_cfg(custom_cfg['noise']['train'])\n",
    "noise_val = normalise_noise_cfg(custom_cfg['noise']['val'])\n",
    "\n",
    "train_dataset = SpectraDataset(\n",
    "    n_samples=n_train,\n",
    "    num_points=n_points,\n",
    "    poly_freq_CH4=poly_freq_CH4,\n",
    "    transitions_dict=transitions_dict,\n",
    "    sample_ranges=train_ranges,\n",
    "    strict_check=True,\n",
    "    with_noise=True,\n",
    "    noise_profile=noise_train,\n",
    "    freeze_noise=False,\n",
    ")\n",
    "val_dataset = SpectraDataset(\n",
    "    n_samples=n_val,\n",
    "    num_points=n_points,\n",
    "    poly_freq_CH4=poly_freq_CH4,\n",
    "    transitions_dict=transitions_dict,\n",
    "    sample_ranges=val_ranges,\n",
    "    strict_check=True,\n",
    "    with_noise=True,\n",
    "    noise_profile=noise_val,\n",
    "    freeze_noise=True,\n",
    ")\n",
    "len(train_dataset), len(val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inspect a sample and denormalise its parameters\n",
    "\n",
    "We extract one training example, recover the physical parameters, and prepare the spectra in both raw and normalised forms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[0]\n",
    "params_norm = sample['params'].unsqueeze(0)\n",
    "params_phys_tensors = denorm_params(params_norm)\n",
    "params_phys = {name: tensor[0].item() for name, tensor in params_phys_tensors.items()}\n",
    "params_phys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reconstruct clean spectra and compare normalisations\n",
    "\n",
    "The helper below rebuilds the clean spectrum from the denormalised parameters. We compare raw and normalised clean/noisy curves to make each normalisation step explicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_raw = regenerate_clean_spectrum(\n",
    "    params_phys,\n",
    "    num_points=n_points,\n",
    "    poly_freq_CH4=poly_freq_CH4,\n",
    "    transitions_dict=transitions_dict,\n",
    ")\n",
    "noisy_raw = sample['noisy_spectra'] * sample['scale']\n",
    "clean_norm = sample['clean_spectra']\n",
    "noisy_norm = sample['noisy_spectra']\n",
    "\n",
    "clean_max = clean_raw.max().item()\n",
    "noisy_max = noisy_raw.max().item()\n",
    "print(f'Clean raw max: {clean_max:.4f}')\n",
    "print(f'Clean normalised max: {clean_norm.max().item():.4f}')\n",
    "print(f'Noisy raw max: {noisy_max:.4f}')\n",
    "print(f'Noisy normalised max: {noisy_norm.max().item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "axes[0].plot(clean_raw.numpy(), label='clean (raw)', linewidth=2)\n",
    "axes[0].plot(noisy_raw.numpy(), label='noisy (raw)', linewidth=1, alpha=0.8)\n",
    "axes[0].set_title('Raw spectra')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylabel('Intensity')\n",
    "\n",
    "axes[1].plot(clean_norm.numpy(), label='clean (normalised)', linewidth=2)\n",
    "axes[1].plot(noisy_norm.numpy(), label='noisy (normalised)', linewidth=1, alpha=0.8)\n",
    "axes[1].set_title('Normalised spectra')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Spectral point index')\n",
    "axes[1].set_ylabel('Scaled intensity')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "- Validation intervals are cloned after overriding the training domain and clamped to guarantee they remain nested.\n",
    "- The registered normalisation parameters align with the narrowed training space so every downstream component sees consistent scaling.\n",
    "- Visualisations confirm that the notebook exposes both the raw and normalised spectra for clean and noisy signals, clarifying the full data-generation pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}