{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Contrôle complet de l'entraînement et d'Optuna\n\nCe notebook démontre comment modifier **tous** les paramètres du pipeline PhysAE : données, modèle, entraînement par stages et recherches Optuna."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Imports\n\nOn rassemble ici toutes les dépendances nécessaires pour instancier le pipeline et lancer des optimisations."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from copy import deepcopy\nfrom pprint import pprint\n\nimport optuna\nimport pytorch_lightning as pl\nimport torch\n\nfrom physae import build_data_and_model, optimise_stage, train_stage_custom\nfrom physae.config_loader import load_data_config, load_stage_config, merge_dicts"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Inspection des configurations de base\n\nLes fichiers YAML fournis servent de point de départ. On peut les afficher et les modifier librement en mémoire."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "data_cfg = load_data_config(name='default')\nstage_A_cfg = load_stage_config('A')\nprint('Résumé data:')\nprint({k: data_cfg[k] for k in ['n_points', 'n_train', 'n_val', 'batch_size']})\nprint('\nHyperparamètres du stage A:')\npprint({k: stage_A_cfg[k] for k in ['epochs', 'base_lr', 'refiner_lr', 'train_base', 'train_heads']})"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Surcharges complètes des données et du modèle\n\nOn peut ajuster le moindre champ : tailles, plages physiques, bruit, architecture du réseau et optimiseur. Cette cellule crée un nouveau lot de loaders et un modèle configuré avec ces modifications."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "data_overrides = {\n    'n_train': 2048,\n    'n_val': 256,\n    'batch_size': 32,\n    'train_ranges': {\n        'sig0': [3085.42, 3085.47],\n        'dsig': [0.0015, 0.0016],\n        'mf_CH4': [3e-6, 5e-5],\n        'baseline0': [0.98, 1.02],\n        'baseline1': [-6e-4, -1.5e-4],\n        'baseline2': [-6e-8, -1.5e-8],\n        'P': [350.0, 700.0],\n        'T': [295.0, 330.0],\n    },\n    'val_ranges': {\n        'sig0': [3085.43, 3085.46],\n        'dsig': [0.00152, 0.00156],\n        'mf_CH4': [5e-6, 3e-5],\n        'baseline0': [0.99, 1.01],\n        'baseline1': [-4e-4, -2.5e-4],\n        'baseline2': [-4.5e-8, -2.5e-8],\n        'P': [380.0, 620.0],\n        'T': [300.0, 315.0],\n    },\n    'noise': {\n        'train': {\n            'std_add_range': [0.0, 0.02],\n            'std_mult_range': [0.0, 0.015],\n            'p_drift': 0.4,\n            'drift_sigma_range': [10.0, 80.0],\n            'drift_amp_range': [0.003, 0.05],\n            'p_fringes': 0.4,\n            'fringe_freq_range': [0.3, 25.0],\n            'fringe_amp_range': [0.001, 0.02],\n            'p_spikes': 0.2,\n            'spikes_count_range': [1, 5],\n            'spike_amp_range': [0.001, 0.4],\n            'spike_width_range': [1.0, 12.0],\n            'clip': [0.0, 1.25],\n        },\n        'val': {\n            'std_add_range': [0.0, 5e-4],\n            'std_mult_range': [0.0, 5e-4],\n            'p_drift': 0.0,\n            'drift_sigma_range': [15.0, 120.0],\n            'drift_amp_range': [0.0, 0.015],\n            'p_fringes': 0.0,\n            'fringe_freq_range': [0.5, 12.0],\n            'fringe_amp_range': [0.0, 0.005],\n            'p_spikes': 0.0,\n            'spikes_count_range': [1, 2],\n            'spike_amp_range': [0.0, 0.02],\n            'spike_width_range': [1.0, 3.0],\n            'clip': [0.0, 1.1],\n        },\n    },\n    'predict_list': ['sig0', 'dsig', 'mf_CH4', 'P', 'T', 'baseline1', 'baseline2'],\n    'film_list': ['sig0', 'P', 'T'],\n    'lrs': [3e-4, 1e-4],\n    'model': {\n        'encoder': {\n            'name': 'efficientnet',\n            'params': {\n                'width_mult': 1.25,\n                'depth_mult': 1.1,\n                'expand_ratio_scale': 1.1,\n                'se_ratio': 0.3,\n                'norm_groups': 8,\n            },\n        },\n        'shared_head_hidden_scale': 0.6,\n        'refiner': {\n            'name': 'efficientnet',\n            'params': {\n                'width_mult': 0.9,\n                'depth_mult': 1.1,\n                'expand_ratio_scale': 1.05,\n                'se_ratio': 0.3,\n                'norm_groups': 8,\n                'hidden_scale': 0.55,\n            },\n        },\n        'optimizer': {\n            'name': 'adamw',\n            'betas': [0.92, 0.999],\n            'weight_decay': 5e-5,\n        },\n        'scheduler': {\n            'eta_min': 1e-8,\n            'T_max': 200,\n        },\n    },\n}\n\nmodel, (train_loader, val_loader), metadata = build_data_and_model(config_overrides=data_overrides)\nprint('Tailles des loaders :', len(train_loader.dataset), len(val_loader.dataset))\nprint('Batch size :', train_loader.batch_size)\nprint('Paramètres à prédire :', metadata['predict_list'])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Paramétrage fin du stage d'entraînement\n\nLa fonction `train_stage_custom` accepte directement des surcharges pour chaque hyperparamètre : l'exemple ci-dessous illustre comment activer/désactiver des blocs, changer d'optimiseur ou ajuster le scheduler."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "stage_overrides = {\n    'stage_name': 'A',\n    'epochs': 8,\n    'base_lr': 3e-4,\n    'refiner_lr': 1e-4,\n    'train_base': True,\n    'train_heads': True,\n    'train_film': True,\n    'train_refiner': True,\n    'refine_steps': 1,\n    'delta_scale': 0.12,\n    'use_film': True,\n    'film_subset': ['sig0', 'P'],\n    'heads_subset': ['sig0', 'dsig', 'mf_CH4'],\n    'baseline_fix_enable': True,\n    'optimizer': 'adamw',\n    'optimizer_weight_decay': 5e-5,\n    'optimizer_beta1': 0.93,\n    'optimizer_beta2': 0.9993,\n    'scheduler_eta_min': 5e-8,\n    'scheduler_T_max': 120,\n    'accelerator': 'cpu',\n    'enable_progress_bar': True,\n    'trainer_kwargs': {\n        'gradient_clip_val': 1.0,\n        'accumulate_grad_batches': 2,\n        'precision': 32,\n    },\n}\n\n# Exemple d'appel (désactivé par défaut pour éviter un entraînement long)\n# train_stage_custom(model, train_loader, val_loader, **stage_overrides)\nprint('Exemple de configuration prête pour train_stage_custom :')\npprint(stage_overrides)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Création d'un espace de recherche Optuna sur mesure\n\nOn peut enrichir les espaces `optuna` en mémoire avant d'appeler `optimise_stage`. Ici on optimise à la fois des hyperparamètres du stage et des plages de données."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "stage_A_search = deepcopy(stage_A_cfg)\nstage_A_search['optuna'].update({\n    'epochs': {'type': 'int', 'low': 6, 'high': 18},\n    'base_lr': {'type': 'float', 'low': 5e-5, 'high': 8e-4, 'log': True},\n    'optimizer_weight_decay': {'type': 'float', 'low': 1e-6, 'high': 1e-4, 'log': True},\n    'data.train_ranges.mf_CH4.low': {'type': 'float', 'low': 2e-6, 'high': 1e-5, 'log': True},\n    'data.train_ranges.mf_CH4.high': {'type': 'float', 'low': 2e-5, 'high': 8e-5, 'log': True},\n    'data.noise.train.std_add_range.high': {'type': 'float', 'low': 0.005, 'high': 0.03},\n})\n\nprint('Espace de recherche Optuna enrichi :')\npprint(stage_A_search['optuna'])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Lancement d'une optimisation Optuna complète\n\nLe bloc suivant montre comment lancer un petit nombre d'essais tout en contrôlant sampler, pruner et répertoires d'artefacts. Adapter `n_trials` et les plages selon vos besoins."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "sampler = optuna.samplers.TPESampler(seed=123)\npruner = optuna.pruners.MedianPruner(n_warmup_steps=1)\n\nstudy = optimise_stage(\n    'A',\n    n_trials=3,\n    metric='val_loss',\n    direction='minimize',\n    data_config_name='default',\n    data_overrides={'n_train': 1024, 'n_val': 128},\n    stage_overrides={'epochs': 6},\n    sampler=sampler,\n    pruner=pruner,\n    output_dir='artifacts/optuna_demo',\n    save_figures=False,\n)\n\nstudy.trials_dataframe()"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}